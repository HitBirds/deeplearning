{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import cbow_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "#dimension of the word embedding vectors\n",
    "EMBED_SIZE = 128\n",
    "#the context window\n",
    "SKIP_WINDOW = 2\n",
    "#number of negative examples to sample\n",
    "NUM_SAMPLED = 64\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "VISUAL_FLD = 'visualization'\n",
    "SKIP_STEP = 5000\n",
    "\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "EXPECTED_BYTES = 31344016\n",
    "#number of tokens to visualize\n",
    "NUM_VISUALIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(dataset):\n",
    "    #Step 1:get input, output from the dataset\n",
    "    with tf.name_scope('data'):\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        around_words, target_words = iterator.get_next()\n",
    "    #Step 2+3:define weights and embedding lookup.\n",
    "    with tf.name_scope('embed'):\n",
    "        embeds = None\n",
    "        embed_matrix = tf.get_variable('embed_matrix',shape=[VOCAB_SIZE,EMBED_SIZE],\n",
    "                                      initializer = tf.random_uniform_initializer())\n",
    "        for i in range(2*SKIP_WINDOW):\n",
    "            embed = tf.nn.embedding_lookup(embed_matrix, around_words[:,i])\n",
    "            print('embedding %d shape: %s'%(i,embed.get_shape().as_list()))\n",
    "            emb_x,emb_y = embed.get_shape().as_list()\n",
    "            if embeds is None:\n",
    "                embeds = tf.reshape(embed,[emb_x,emb_y,1])\n",
    "            else:\n",
    "                embeds = tf.concat([embeds,tf.reshape(embed,[emb_x,emb_y,1])],2)\n",
    "        assert embeds.get_shape().as_list()[2]==2*SKIP_WINDOW\n",
    "        avg_embed =  tf.reduce_mean(embeds,2,keep_dims=False)\n",
    "    #Step 4: construct variables for NCE loss and define loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        softmax_weights = tf.get_variable('nce_weight',shape=[VOCAB_SIZE,EMBED_SIZE],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=1.0/(EMBED_SIZE**0.5)))\n",
    "        softmax_biases = tf.get_variable('nce_bias',initializer=tf.zeros([VOCAB_SIZE]))\n",
    "        #define loss function to be SM loss function\n",
    "        loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights,\n",
    "                                                         biases = softmax_biases,\n",
    "                                                         inputs = avg_embed,\n",
    "                                                         labels = target_words,\n",
    "                                                         num_sampled = NUM_SAMPLED,\n",
    "                                                         num_classes = VOCAB_SIZE),name='loss')\n",
    "    #Step 5:define optimizer\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "        utils.safe_mkdir('checkpoints2')\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(iterator.initializer)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            total_loss = 0.0\n",
    "            writer = tf.summary.FileWriter('graphs/word2vec_simple',sess.graph)\n",
    "            for index in range(NUM_TRAIN_STEPS):\n",
    "                try:\n",
    "                    loss_batch, _ = sess.run([loss,optimizer])\n",
    "                    total_loss+=loss_batch\n",
    "                    if (index + 1) % SKIP_STEP == 0:\n",
    "                        print('Average loss at step {}:{:5.1f}'.format(index,total_loss/SKIP_STEP))\n",
    "                        total_loss = 0.0\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    sess.run(iterator.initializer)\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding 0 shape: [128, 128]\n",
      "embedding 1 shape: [128, 128]\n",
      "embedding 2 shape: [128, 128]\n",
      "embedding 3 shape: [128, 128]\n",
      "data/text8.zip already exists\n",
      "Average loss at step 4999:  4.0\n",
      "Average loss at step 9999:  3.6\n",
      "Average loss at step 14999:  3.5\n",
      "Average loss at step 19999:  3.4\n",
      "Average loss at step 24999:  3.4\n",
      "Average loss at step 29999:  3.3\n",
      "Average loss at step 34999:  3.3\n",
      "Average loss at step 39999:  3.3\n",
      "Average loss at step 44999:  3.3\n",
      "Average loss at step 49999:  3.2\n",
      "Average loss at step 54999:  3.2\n",
      "Average loss at step 59999:  3.2\n",
      "Average loss at step 64999:  3.2\n",
      "Average loss at step 69999:  3.1\n",
      "Average loss at step 74999:  3.1\n",
      "Average loss at step 79999:  3.1\n",
      "Average loss at step 84999:  3.1\n",
      "Average loss at step 89999:  3.1\n",
      "Average loss at step 94999:  3.1\n",
      "Average loss at step 99999:  3.0\n"
     ]
    }
   ],
   "source": [
    "def gen():\n",
    "    yield from cbow_gen.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE, \n",
    "                                        BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)\n",
    "\n",
    "def main():\n",
    "    dataset = tf.data.Dataset.from_generator(gen, \n",
    "                                (tf.int32, tf.int32), \n",
    "                                (tf.TensorShape([BATCH_SIZE,SKIP_WINDOW*2]), tf.TensorShape([BATCH_SIZE, 1])))\n",
    "    word2vec(dataset)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
